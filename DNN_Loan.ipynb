{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "DNN_Loan.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sprivacy/DP-Utility-in-ML/blob/main/DNN_Loan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08AqFBVBuPpc"
      },
      "source": [
        "#Mount Data on your google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmML9EBW7QGY"
      },
      "source": [
        "#Neccessary Libraries for DP-SGD\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 1.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "tf.compat.v1.logging.set_verbosity(tf.logging.ERROR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QK_2T7bGvKyS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3853128f-5d2b-4e6c-ff11-1a644dda6e34"
      },
      "source": [
        "#importing libraries\n",
        "#from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy\n",
        "#from tensorflow_privacy.privacy.optimizers.dp_optimizer import DPGradientDescentGaussianOptimizer,DPAdamGaussianOptimizer\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "from matplotlib import pyplot\n",
        "from keras.datasets import cifar10\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.optimizers import SGD\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfBC6QxZrI8t"
      },
      "source": [
        "#install tensorflow privacy for DP-SGD\n",
        "!pip install tensorflow_privacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIt8g3DBqcwB"
      },
      "source": [
        "\n",
        "from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy\n",
        "from tensorflow_privacy.privacy.optimizers.dp_optimizer import DPGradientDescentGaussianOptimizer,DPAdamGaussianOptimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRC9uE4ovqwC"
      },
      "source": [
        "dataset = pd.read_csv('...../accepted_2007_to_2018Q4.csv.gz', low_memory=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feU17p28vzPV"
      },
      "source": [
        "#visualize the dataset\n",
        "#dataset.head(10)\n",
        "data=dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2gdRGX2v0Cg"
      },
      "source": [
        "#Drop null data\n",
        "data = data.sample(frac=0.2, axis=0, random_state=42).reset_index(drop=True)\n",
        "data = data.drop(data.loc[:, data.isna().mean().sort_values() > 0.3].columns, axis=1)\n",
        "data = data.dropna(axis=0).reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bJNqAzPv3Uv"
      },
      "source": [
        "#Drop un-needed column\n",
        "unneeded_columns = ['id', 'sub_grade', 'emp_title', 'url', 'title']\n",
        "{column: list(data[column].unique()) for column in data.drop(unneeded_columns, axis=1).columns if data.dtypes[column] == 'object'}\n",
        "data = data.drop(unneeded_columns, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtEMQ9Gbv52g"
      },
      "source": [
        "#dealing with date feature\n",
        "date_columns = ['issue_d', 'earliest_cr_line', 'last_pymnt_d', 'last_credit_pull_d']\n",
        "for column in date_columns:\n",
        "    data[column + '_month'] = data[column].apply(lambda x: x[0:3])\n",
        "    data[column + '_year'] = data[column].apply(lambda x: x[-4:])\n",
        "\n",
        "data = data.drop(date_columns, axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsMi_oypwAqK"
      },
      "source": [
        "month_ordering = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
        "for column in date_columns:\n",
        "    data[column + '_month'] = data[column + '_month'].apply(lambda x: month_ordering.index(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-1x77FWwEZF"
      },
      "source": [
        "{column: list(data[column].unique()) for column in data.columns if data.dtypes[column] == 'object'}\n",
        "target = 'grade'\n",
        "\n",
        "binary_features = ['term', 'pymnt_plan', 'initial_list_status', 'application_type', 'hardship_flag', 'disbursement_method', 'debt_settlement_flag']\n",
        "binary_positives = [' 60 months', 'y', 'w', 'Individual', 'Y', 'Cash', 'Y']\n",
        "ordinal_features = ['emp_length']\n",
        "emp_ordering = [\n",
        "    '< 1 year',\n",
        "    '1 year',\n",
        "    '2 years',\n",
        "    '3 years',\n",
        "    '4 years',\n",
        "    '5 years',\n",
        "    '6 years',\n",
        "    '7 years',\n",
        "    '8 years',\n",
        "    '9 years',\n",
        "    '10+ years'\n",
        "]\n",
        "\n",
        "nominal_features = ['home_ownership', 'verification_status', 'loan_status', 'purpose', 'addr_state']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxrKcIsUwHpN"
      },
      "source": [
        "# Encoding functions\n",
        "\n",
        "def binary_encode(df, column, positive_value):\n",
        "    df[column] = df[column].apply(lambda x: 1 if x == positive_value else 0)\n",
        "\n",
        "def ordinal_encode(df, column, ordering):\n",
        "    df[column] = df[column].apply(lambda x: ordering.index(x))\n",
        "\n",
        "def onehot_encode(df, column):\n",
        "    dummies = pd.get_dummies(df[column])\n",
        "    df_new = pd.concat([df, dummies], axis=1)\n",
        "    df_new = df_new.drop(column, axis=1)\n",
        "    return df_new"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6CCYs3ewLQE"
      },
      "source": [
        "# Perform encoding\n",
        "for feature, positive_value in zip(binary_features, binary_positives):\n",
        "    binary_encode(data, feature, positive_value)\n",
        "\n",
        "ordinal_encode(data, 'emp_length', emp_ordering)\n",
        "\n",
        "for feature in nominal_features:\n",
        "    data = onehot_encode(data, feature)\n",
        "\n",
        "# Encoding label column\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "data[target] = label_encoder.fit_transform(data[target])\n",
        "\n",
        "target_mappings = {index: label for index, label in enumerate(label_encoder.classes_)}\n",
        "target_mappings\n",
        "#remove string from zipcode/convert it into integers\n",
        "data['zip_code'] = data['zip_code'].str.replace(r'\\D', '').astype(int)\n",
        "y = data['grade']\n",
        "X = data.drop('grade', axis=1)\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X[0:100000], y[0:100000], train_size=0.5, random_state=0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sl4E_0c91yB0"
      },
      "source": [
        "x_train=X_train.reshape(X_train.shape[0],len(X_train[0]),1)\n",
        "x_test=X_test.reshape(X_test.shape[0],len(X_test[0]),1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfOb0C6XmgnM"
      },
      "source": [
        "y_train=to_categorical(y_train)\n",
        "y_test=to_categorical(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTywpBh4Xawf"
      },
      "source": [
        "def label(my_list):\n",
        "  import numpy as np\n",
        "  my_array=np.array(my_list)\n",
        "  p=np.zeros(my_array.shape)\n",
        "  b=my_array.max(-1)\n",
        "  condition = my_array == b[..., np.newaxis]\n",
        "  c = np.where(condition, 1, 0)\n",
        "  final=np.multiply(c, my_array)\n",
        "  #my_sum=np.sum(final,axis=0)\n",
        "  labels=np.argmax(final, axis=1)\n",
        "  return labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYTJkblxzJ41"
      },
      "source": [
        "Input Perturbation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4z7cfg5dzL2F"
      },
      "source": [
        "n=25000\n",
        "xtrain=x_train[0:n]\n",
        "ytrain=y_train[0:n]\n",
        "xtest=x_test[0:n]\n",
        "ytest=y_test[0:n]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KYSpm2fzOWJ"
      },
      "source": [
        "from keras.regularizers import l2\n",
        "def Model(x_train,y_train,x_test,y_test):\n",
        "   opt = tf.keras.optimizers.Adam(learning_rate=.01)\n",
        "   model_np = tf.keras.Sequential([\n",
        "   #tf.keras.layers.Flatten(),\n",
        "   tf.keras.layers.Dense(64,\n",
        "                           activation='relu',\n",
        "                           input_shape=(166,1)),\n",
        "   #tf.keras.layers.MaxPool2D(2, 1),\n",
        "   tf.keras.layers.Dense(64,\n",
        "                           activation='relu'),\n",
        "   #tf.keras.layers.MaxPool2D(2, 1),\n",
        "   tf.keras.layers.Flatten(),\n",
        "   #tf.keras.layers.Dense(50,activation='relu'),\n",
        "   #tf.keras.layers.Dense(25, activation='relu'),\n",
        "   tf.keras.layers.Dense(7, activation='softmax')\n",
        "       ])\n",
        "   loss = tf.keras.losses.CategoricalCrossentropy(\n",
        "    from_logits=True, reduction=tf.losses.Reduction.NONE)\n",
        "   model_np.compile(optimizer=opt, loss=loss, metrics=['accuracy'])\n",
        "   model_np.fit(x_train, y_train,\n",
        "          epochs=100,\n",
        "          validation_data=(x_test[0:1000], y_test[0:1000]),\n",
        "          batch_size=250)\n",
        "   return model_np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbYWpQQTzSLY"
      },
      "source": [
        "def calc_labelAcc(Model,xtest,ytest):\n",
        "  from sklearn.metrics import accuracy_score\n",
        "  ypred=Model.predict(xtest)\n",
        "  acc=accuracy_score(label(ytest), label(ypred))\n",
        "  return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svETDM_6zVKD"
      },
      "source": [
        "#non-private model\n",
        "model_np=Model(xtrain,ytrain,xtest,ytest)\n",
        "acc_np=calc_labelAcc(model_np,xtest,ytest)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDqDdlNmzaiG"
      },
      "source": [
        "def steps(L,E,n):\n",
        "  q=L/n\n",
        "  T=E/q\n",
        "  return T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMkTmD29zgpF"
      },
      "source": [
        "ep=[0.01, 0.1,1,10,100,1000,10000]\n",
        "ni=len(ep)\n",
        "acc_in=np.zeros(ni)\n",
        "for i in range(ni):\n",
        "  epi=ep[i]\n",
        "  delta=.00001\n",
        "  c=1\n",
        "  L=250\n",
        "  E=100\n",
        "  T=steps(L,E,n)\n",
        "  #print(T)\n",
        "  G=3*np.log(n)\n",
        "  z1=c*(G**2)*T*np.log(1/delta)\n",
        "  z2=n*(n-1)*(epi**2)\n",
        "  sigma=z1/z2\n",
        "  nsi=np.random.normal(loc=0.0, scale=sigma)\n",
        "  print(nsi)\n",
        "  X_in=xtrain+nsi\n",
        "  model_in=Model(X_in,ytrain,xtest,ytest)\n",
        "  acc_in[i]=calc_labelAcc(model_in,xtest,ytest)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGpB7L9hzjYY"
      },
      "source": [
        "util_in=acc_np-acc_in\n",
        "util_in"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIf5rWa7zQSl"
      },
      "source": [
        "Gradient_Perturbation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0d6Su1Te4ss"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ReduceLROnPlateau\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cipdYUIO0M9r"
      },
      "source": [
        "def DP_SGD(noise_multiplier,x_train,y_train,x_test,y_test):\n",
        "   epochs = 100\n",
        "   batch_size = 250\n",
        "   l2_norm_clip = 1.5\n",
        "   #noise_multiplier = .88\n",
        "   num_microbatches = 125\n",
        "   learning_rate = 0.01\n",
        "   x_train_s=x_train\n",
        "   y_train_s=y_train\n",
        "   x_test=x_test\n",
        "   y_test_s=y_test\n",
        "   n=len(y_train)\n",
        "   #noise_multiplier=.5\n",
        "   if batch_size % num_microbatches != 0:\n",
        "     raise ValueError('Batch size should be an integer multiple of the number of microbatches')\n",
        "   \n",
        "   eps=compute_dp_sgd_privacy.compute_dp_sgd_privacy(n=n, batch_size=250, noise_multiplier=noise_multiplier, epochs=epochs, delta=1e-5)\n",
        "\n",
        "   model = tf.keras.Sequential([\n",
        "   tf.keras.layers.Dense(64,\n",
        "                           activation='relu',\n",
        "                           input_shape=(166,1)),\n",
        "   tf.keras.layers.Dense(64,\n",
        "                           activation='relu'),\n",
        "   #tf.keras.layers.MaxPool2D(2, 1),\n",
        "   tf.keras.layers.Flatten(),\n",
        "   tf.keras.layers.Dense(7, activation='softmax')\n",
        "       ])\n",
        "   print(\"compplete\")\n",
        "   optimizer = DPAdamGaussianOptimizer(\n",
        "            l2_norm_clip=l2_norm_clip,\n",
        "            noise_multiplier=noise_multiplier,\n",
        "            num_microbatches=num_microbatches,\n",
        "            learning_rate=learning_rate)\n",
        "   \n",
        "   loss = tf.keras.losses.CategoricalCrossentropy(\n",
        "    from_logits=True, reduction=tf.losses.Reduction.NONE)\n",
        "   model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "   model.fit(x_train_s, y_train_s,\n",
        "          epochs=epochs,\n",
        "          validation_data=(x_test[0:1000], y_test_s[0:1000]),\n",
        "          batch_size=batch_size)\n",
        "   return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m78O_oB5zKl0"
      },
      "source": [
        "#model=SGD_lg(1,x_train[0:15000],trainY[0:15000],x_test,testY)\n",
        "def calc_labelAcc(Model,xtest,ytest):\n",
        "  from sklearn.metrics import accuracy_score\n",
        "  ypred=Model.predict_classes(xtest)\n",
        "  acc=accuracy_score(label(ytest), ypred)\n",
        "  return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRLnTz9DzOkN"
      },
      "source": [
        "###training and testing data\n",
        "n=25000\n",
        "xtrain_agg=x_train[0:n]\n",
        "ytrain_agg=y_train[0:n]\n",
        "#ytrain_agg=ytrain_agg.to_numpy()\n",
        "#ytrain_pred=model_np.predict_proba(xtrain_agg)\n",
        "xtest_agg=x_test[0:n]\n",
        "ytest_agg=y_test[0:n]\n",
        "#ytest_agg=ytest_agg.to_numpy()\n",
        "#ytest_pred=model_np.predict_proba(xtest_agg)\n",
        "target_train_agg = (xtrain_agg,ytrain_agg)\n",
        "target_test_agg = (xtest_agg,ytest_agg)\n",
        "#target_train_data_agg, target_test_data_agg = sample_data(target_train_agg, target_test_agg, NUM_TARGET)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Y_PdQV-ztN9"
      },
      "source": [
        "noise_mul=[350,33,4,.831,.41,.25515,.1856]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPWTC77t379D"
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "acc_sgd= np.zeros(len(noise_mul))\n",
        "model_index =0\n",
        "for i in range(7):\n",
        "  model=DP_SGD(noise_mul[i],xtrain_agg,ytrain_agg,xtest_agg,ytest_agg)\n",
        "  acc_sgd[i]=calc_labelAcc(model,xtest_agg,ytest_agg)\n",
        "  globals()['model_sgd%s' % i]=model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFS-JfkIJjSH"
      },
      "source": [
        "#For non-private Settings\n",
        "from keras.regularizers import l2\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=.01)\n",
        "model_np = tf.keras.Sequential([\n",
        "   #tf.keras.layers.Flatten(),\n",
        "   tf.keras.layers.Dense(64,\n",
        "                           activation='relu',\n",
        "                           input_shape=(166,1)),\n",
        "   #tf.keras.layers.MaxPool2D(2, 1),\n",
        "   tf.keras.layers.Dense(64,\n",
        "                           activation='relu'),\n",
        "   #tf.keras.layers.MaxPool2D(2, 1),\n",
        "   tf.keras.layers.Flatten(),\n",
        "   #tf.keras.layers.Dense(50,activation='relu'),\n",
        "   #tf.keras.layers.Dense(25, activation='relu'),\n",
        "   tf.keras.layers.Dense(7, activation='softmax')\n",
        "       ])\n",
        "loss = tf.keras.losses.CategoricalCrossentropy(\n",
        "    from_logits=True, reduction=tf.losses.Reduction.NONE)\n",
        "model_np.compile(optimizer=opt, loss=loss, metrics=['accuracy'])\n",
        "model_np.fit(xtrain_agg, ytrain_agg,\n",
        "          epochs=100,\n",
        "          validation_data=(x_test[0:1000], y_test[0:1000]),\n",
        "          batch_size=250)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1qH8RgwOb6E"
      },
      "source": [
        "np_sgd=calc_labelAcc(model_np,xtest_agg,ytest_agg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_Xlblo_H-lj"
      },
      "source": [
        "PATE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ro8sN2uJIALF"
      },
      "source": [
        "def build_cnn_model():\n",
        "    import keras\n",
        "    from keras.models import Sequential\n",
        "    from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "    num_class=7\n",
        "    # build the model\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64, activation='relu', input_shape=(len(x_train[0]),1)))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Flatten())\n",
        "    #model.add(Dense(50, activation='relu'))\n",
        "    #model.add(Dense(64, activation='tanh'))\n",
        "    if num_class==1:\n",
        "        model.add(Dense(num_class, activation='sigmoid'))    \n",
        "    else:\n",
        "        model.add(Dense(num_class, activation='softmax')) \n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CASCg8c4lGUU",
        "outputId": "b571ee98-a78e-4ea3-ff60-76890c5ac914"
      },
      "source": [
        "#split datasets into multiple teacher\n",
        "#Divide the images into 5 \n",
        "# split x_train to 10 disjoint datasets\n",
        "#store each dataset variable name is the list Xtrain\n",
        "import numpy as np\n",
        "M=40\n",
        "j=0\n",
        "k=len(xtrain_agg)/M\n",
        "#print(x_train)\n",
        "\n",
        "for x in range(0,M):\n",
        "             globals()['x_train_split%s' % x]=xtrain_agg[int(j):int(k+j)]\n",
        "             globals()['y_train_split%s' % x]=ytrain_agg[int(j):int(k+j)]\n",
        "             j=k+j\n",
        "             #print(j)\n",
        "             \n",
        "print(x_train_split29.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(625, 166, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HB1rcc7rlkAD"
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "model_index =0\n",
        "for iter in range(M):\n",
        "  model=build_cnn_model()\n",
        "  model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  model.fit(globals()['x_train_split%s' % iter], globals()['y_train_split%s' % iter], batch_size=250, epochs=100, verbose=1, shuffle=True)\n",
        "  #save the models\n",
        "  globals()['model_pate%s' % iter]=model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W61JNSuNoNNf"
      },
      "source": [
        "#function for vote counting\n",
        "def vote(my_list):\n",
        "  import numpy as np\n",
        "  my_array=np.array(my_list)\n",
        "  p=np.zeros(my_array.shape)\n",
        "  b=my_array.max(-1)\n",
        "  condition = my_array == b[..., np.newaxis]\n",
        "  c = np.where(condition, 1, 0)\n",
        "  final=np.multiply(c, my_array)\n",
        "  #my_sum=np.sum(final,axis=0)\n",
        "  labels=np.argmax(final, axis=1)\n",
        "  return c"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Py3-JfirVV6"
      },
      "source": [
        "sum=np.zeros([len(y_test),7])\n",
        "for i in range(M):\n",
        "  New_model=globals()['model_pate%s' % i]\n",
        "  y=np.zeros(y_test.shape)\n",
        "  y=New_model.predict_proba(x_test)\n",
        "  y=vote(y)\n",
        "  sum=y+sum\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3Ca6WgbrY9L"
      },
      "source": [
        "#create the labels\n",
        "from sklearn.metrics import accuracy_score\n",
        "lab=label(sum)\n",
        "#clear accuracy\n",
        "y_true=label(y_test)\n",
        "np_pate=accuracy_score(y_true,lab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5BsR6K2rfRJ"
      },
      "source": [
        "ep=[.01,.1,1,10,100,1000,10000]\n",
        "client_acc=np.zeros(len(ep))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2u3uKPvwtrf"
      },
      "source": [
        "#add noise \n",
        "def add_noise_sum(noise,sum,experiment,ypred):\n",
        "  sum_s=sum\n",
        "  predt=np.zeros(experiment)\n",
        "  for i1 in range(experiment):\n",
        "    sum_s=np.zeros(sum.shape)\n",
        "    sum_f=sum+np.random.laplace(loc=0.0, scale=1/noise)\n",
        "    sum2=label(sum_f)\n",
        "    predt[i1]=accuracy_score(ypred,sum2)\n",
        "    #print(i)\n",
        "  pred=np.average(predt)\n",
        "  #print(predt)\n",
        "  #print(pred)\n",
        "  return pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDkuuW3dw2ju"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import average_precision_score\n",
        "#Noise after Aggregation Method\n",
        "for i in range(0,len(ep)):\n",
        "                          sum_s=np.zeros(sum.shape)\n",
        "                          sum_n=sum\n",
        "                          #sum_s=sum_n+np.random.laplace(loc=0.0, scale=1/noise[i])\n",
        "                          predF=add_noise_sum(ep[i],sum_n,100,y_true)\n",
        "                          client_acc[i]=predF\n",
        "print(client_acc)\n",
        "print(ep)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lW2BZ0QHAVQ0"
      },
      "source": [
        "array([0.52064, 0.45916, 0.36096, 0.11152, 0.06604, 0.18756, 0.0352 ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrdhJ4DTAcay"
      },
      "source": [
        "array([0.2409964, 0.1150398, 0.0465926, 0.       , 0.       , 0.       ,\n",
        "       0.       ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVHniCfhw5oC"
      },
      "source": [
        "Graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4Rf_fyZw66a"
      },
      "source": [
        "import pandas as pd \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns #grafikleştirme için\n",
        "import matplotlib.pyplot as plt \n",
        "from google.colab import files\n",
        "test1 = plt.figure()\n",
        "#plt.semilogx(ep,non_p-acc_obj,color=\"black\",marker='3',label='Objective Perturbation',linewidth=1.5)\n",
        "plt.semilogx(ep,util_in,color=\"black\",marker='^',label='Input Perturbation',linewidth=1.5)\n",
        "plt.semilogx(ep,util_sgd,color=\"red\",marker='*',label='Gradient Perturbation',linewidth=1.5)\n",
        "#plt.semilogx(ep,np_out-acc_out,color=\"black\",marker='+',linestyle=\"--\",label='Output Perturbation',linewidth=1.5)\n",
        "plt.semilogx(ep,util_pate,color=\"brown\",marker='o',label='Prediction Perturbation',linewidth=1.5)\n",
        "#plt.semilogx(ep,non_p-acc_in,color=\"orange\",marker='.',linestyle=\"--\",label='Input',linewidth=1.5)\n",
        "#plt.plot(ep,non_p,color=\"red\",marker='*',linestyle=\"--\",label='Non-Private Model',linewidth=2)\n",
        "plt.legend(loc=1,fontsize=12)\n",
        "plt.xlabel(\"Privacy Budget($\\epsilon$)\",fontsize=13)\n",
        "plt.ylabel(\"Utility Loss\",fontsize=15)\n",
        "#plt.xscale('symlog', linthreshy=0.1)\n",
        "#plt.ylim([-.1,1])\n",
        "plt.xticks(size = 10)\n",
        "plt.yticks(size = 8)\n",
        "plt.ylim([-.05,1])\n",
        "#y.set_color(\"black\")\n",
        "plt.rcParams[\"axes.edgecolor\"] = \"black\"\n",
        "plt.rcParams[\"axes.linewidth\"] = 1\n",
        "plt.rcParams['axes.facecolor'] = 'white'\n",
        "#test1.set_facecolor('white')\n",
        "test1.show()\n",
        "test1.savefig('DNN_perturb_acc_loan.pdf')\n",
        "files.download('DNN_perturb_acc_loan.pdf')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PP7LfsvpZyo1"
      },
      "source": [
        "Attack the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xu8A5r1Ydcrd"
      },
      "source": [
        "#Assign necessary variables for attacking the model\n",
        "import argparse\n",
        "import csv\n",
        "import numpy as np\n",
        "from sklearn.utils import resample, shuffle\n",
        "\n",
        "import keras\n",
        "import numpy as np\n",
        "from sklearn.utils import resample\n",
        "\n",
        "LEARNING_RATE = 0.001\n",
        "#EPOCH = 100\n",
        "EPOCH = 100\n",
        "DATA_SIZE = 50000\n",
        "TRAINING_SIZE = 25000\n",
        "TEST_SIZE = 25000\n",
        "NUM_TARGET = 1\n",
        "#NUM_SHADOW = 100\n",
        "NUM_SHADOW = 10\n",
        "IN = 1\n",
        "OUT = 0\n",
        "VERBOSE = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_42kFn1Ri4as"
      },
      "source": [
        "#call required libraries\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.utils import to_categorical \n",
        "import sys\n",
        "from matplotlib import pyplot\n",
        "from keras.datasets import cifar10\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.optimizers import SGD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyzGk4Ugi8ZP"
      },
      "source": [
        "#Define model configuaration\n",
        "# Model configuration\n",
        "batch_size = 250\n",
        "#img_width, img_height, img_num_channels = 32, 32, 3\n",
        "#loss_function = sparse_categorical_crossentropy\n",
        "no_classes = 7\n",
        "no_epochs = 250\n",
        "optimizer = Adam()\n",
        "validation_split = 0.2\n",
        "verbosity = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwVvY_JWjD6-"
      },
      "source": [
        "#data sampling\n",
        "def sample_data(train_data,test_data,num_sets):\n",
        "    (x_train, y_train), (x_test, y_test) = train_data, test_data\n",
        "    new_x_train, new_y_train = [], []\n",
        "    new_x_test, new_y_test = [], []\n",
        "    for i in range(num_sets):\n",
        "        x_temp, y_temp = resample(x_train, y_train, n_samples=len(y_train), random_state=0)\n",
        "        new_x_train.append(x_temp)\n",
        "        new_y_train.append(y_temp)\n",
        "        x_temp, y_temp = resample(x_test, y_test, n_samples=len(y_test), random_state=0)\n",
        "        new_x_test.append(x_temp)\n",
        "        new_y_test.append(y_temp)\n",
        "    return (new_x_train, new_y_train), (new_x_test, new_y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JG8O8wZxjiVc"
      },
      "source": [
        "def get_trained_keras_models(keras_model, train_data, test_data, num_models):\n",
        "    (x_train, y_train), (x_test, y_test) = train_data, test_data\n",
        "    models = []\n",
        "    for i in range(num_models):\n",
        "        models.append(keras.models.clone_model(keras_model))\n",
        "        models[i].compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "        models[i].fit(x_train[i], y_train[i], batch_size=250, epochs=EPOCH, verbose=VERBOSE, shuffle=True)\n",
        "        score = models[i].evaluate(x_test[i], y_test[i], verbose=VERBOSE)\n",
        "        print('\\n', 'Model ', i, ' test accuracy:', score[1])\n",
        "    return models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lz93bItykJAQ"
      },
      "source": [
        "#collect the attack dataset from shadow models\n",
        "def get_attack_dataset(models, train_data, test_data, num_models, data_size):\n",
        "    # generate dataset for the attack model\n",
        "    (x_train, y_train), (x_test, y_test) = train_data, test_data\n",
        "    num_classes = 7#len(y_train[0][0])\n",
        "    x_data, y_data = [[] for i in range(num_classes)], [[] for i in range(num_classes)]\n",
        "    for i in range(num_models):\n",
        "        # IN data\n",
        "        x_temp, y_temp = resample(x_train[i], y_train[i], n_samples=data_size, random_state=0)\n",
        "        for j in range(data_size):\n",
        "            y_idx = np.argmax(y_temp[j])\n",
        "            x_data[y_idx].append(models[i].predict(x_temp[j:j+1])[0])\n",
        "            #print(y_idx)\n",
        "            y_data[y_idx].append(IN)\n",
        "            print(\"starts1\",j)\n",
        "        # OUT data\n",
        "        x_temp, y_temp = resample(x_test[i], y_test[i], n_samples=data_size, random_state=0)\n",
        "        for j in range(data_size):\n",
        "            y_idx = np.argmax(y_temp[j])\n",
        "            p=models[i].predict(x_temp[j:j+1])[0]\n",
        "            x_data[y_idx].append(p)\n",
        "            y_data[y_idx].append(OUT)\n",
        "            print(\"starts2\",j)\n",
        "    return x_data, y_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brCxKKynooaL"
      },
      "source": [
        "#collect the attack dataset from target models\n",
        "def get_target_dataset(models, train_data, test_data, num_models, data_size):\n",
        "    # generate dataset for the attack model\n",
        "    (x_train, y_train), (x_test, y_test) = train_data, test_data\n",
        "    num_classes = 7#len(y_train[0][0])\n",
        "    x_data, y_data = [[] for i in range(num_classes)], [[] for i in range(num_classes)]\n",
        "    for i in range(num_models):\n",
        "        # IN data\n",
        "        x_temp, y_temp = resample(x_train[i], y_train[i], n_samples=data_size, random_state=0)\n",
        "        for j in range(data_size):\n",
        "            y_idx = np.argmax(y_temp[j])\n",
        "            x_data[y_idx].append(models[i].predict(x_temp[j:j+1])[0])\n",
        "            #print(y_idx)\n",
        "            y_data[y_idx].append(IN)\n",
        "            print(\"starts1\",j)\n",
        "        # OUT data\n",
        "        x_temp, y_temp = resample(x_test[i], y_test[i], n_samples=data_size, random_state=0)\n",
        "        for j in range(data_size):\n",
        "            y_idx = np.argmax(y_temp[j])\n",
        "            p=models[i].predict(x_temp[j:j+1])[0]\n",
        "            x_data[y_idx].append(p)\n",
        "            y_data[y_idx].append(OUT)\n",
        "            print(\"starts2\",j)\n",
        "    return x_data, y_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDRc3xUklbRI"
      },
      "source": [
        "#Get the prediction vectors from the DP model\n",
        "def get_attack_dataset_combined(x_train, x_test, train_pred, y_train, y_test, test_pred):\n",
        "    # generate dataset for the attack model\n",
        "    #(x_train, y_train), (x_test, y_test) = train_data, test_data\n",
        "    data_size=len(y_train)\n",
        "    num_class = 7\n",
        "    x_data, y_data = [[] for _ in range(num_class)], [[] for _ in range(num_class)]\n",
        "    #for i in range(num_models):\n",
        "        # IN data\n",
        "    #x_temp, y_temp = resample(x_train, y_train, n_samples=data_size, random_state=0)\n",
        "    for j in range(data_size):\n",
        "            y_idx = np.argmax(y_train[j])\n",
        "            x_data[y_idx].append(train_pred[j])\n",
        "            #print(train_pred[j])\n",
        "            #x_data[y_idx].append(models.predict(x_temp[j:j+1])[0])\n",
        "            y_data[y_idx].append(IN)\n",
        "        # OUT data\n",
        "    #x_temp, y_temp = resample(x_test, y_test, n_samples=data_size, random_state=0)\n",
        "    for j in range(data_size):\n",
        "            y_idx = np.argmax(y_test[j])\n",
        "            #x_data[y_idx].append(models.predict(x_temp[j:j+1])[0])\n",
        "            x_data[y_idx].append(test_pred[j])\n",
        "            y_data[y_idx].append(OUT)\n",
        "    return x_data, y_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7V48ZhfJo5Ws"
      },
      "source": [
        "def build_cnn_model():\n",
        "    import keras\n",
        "    from keras.models import Sequential\n",
        "    from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "    num_class=7\n",
        "    # build the model\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64, activation='relu', input_shape=(len(x_train[0]),1)))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Flatten())\n",
        "    #model.add(Dense(50, activation='relu'))\n",
        "    #model.add(Dense(64, activation='tanh'))\n",
        "    if num_class==1:\n",
        "        model.add(Dense(num_class, activation='sigmoid'))    \n",
        "    else:\n",
        "        model.add(Dense(num_class, activation='softmax')) \n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfvJgl5ilRl6"
      },
      "source": [
        "#generate the report\n",
        "def get_leakage(models, test_data):\n",
        "    from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "    from sklearn.metrics import average_precision_score\n",
        "    from sklearn import metrics\n",
        "    from sklearn.metrics import confusion_matrix\n",
        "    (x_test, y_true) = test_data\n",
        "    acc_scores = []\n",
        "    pre_scores = []\n",
        "    rec_scores = []\n",
        "    fp=np.zeros(len(models))\n",
        "    tp=np.zeros(len(models))\n",
        "    tn=np.zeros(len(models))\n",
        "    fn=np.zeros(len(models))\n",
        "    for i in range(len(models)):\n",
        "        y_pred = models[i].predict(x_test[i])\n",
        "        # _LOG_PRINT(y_pred)\n",
        "        acc_scores.append(accuracy_score(y_true[i], y_pred))\n",
        "        pre_scores.append(average_precision_score(y_true[i], y_pred))\n",
        "        rec_scores.append(recall_score(y_true[i], y_pred))\n",
        "        tn[i], fp[i], fn[i], tp[i] = confusion_matrix(y_true[i], y_pred).ravel()\n",
        "    return np.sum(tn),np.sum(tp),np.sum(fn),np.sum(fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObOyjfUDZoYM"
      },
      "source": [
        "#generate the report\n",
        "def get_lkg(models, test_data):\n",
        "    from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "    from sklearn.metrics import average_precision_score\n",
        "    from sklearn import metrics\n",
        "    from sklearn.metrics import confusion_matrix\n",
        "    (x_test, y_true) = test_data\n",
        "    acc_scores = []\n",
        "    pre_scores = []\n",
        "    rec_scores = []\n",
        "    fpr=np.zeros(len(models))\n",
        "    tpr=np.zeros(len(models))\n",
        "    #tn=np.zeros(len(models))\n",
        "    #fn=np.zeros(len(models))\n",
        "    for i in range(len(models)):\n",
        "        y_pred = models[i].predict(x_test[i])\n",
        "        # _LOG_PRINT(y_pred)\n",
        "        acc_scores.append(accuracy_score(y_true[i], y_pred))\n",
        "        pre_scores.append(average_precision_score(y_true[i], y_pred))\n",
        "        rec_scores.append(recall_score(y_true[i], y_pred))\n",
        "        fpr[i], tpr[i],t = confusion_matrix(y_true[i], y_pred)\n",
        "    return np.sum(tpr),np.sum(fpr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UY9Zejkkl0HL"
      },
      "source": [
        "n=25000\n",
        "shadow_train = (x_train[n:n*2],y_train[n:n*2])\n",
        "shadow_test = (x_test[n:n*2],y_test[n:n*2])\n",
        "shadow_train_data, shadow_test_data = sample_data(shadow_train, shadow_test, NUM_SHADOW)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugq2RSm0njc-"
      },
      "source": [
        "cnn_model=build_cnn_model()\n",
        "# compile the shadow models\n",
        "shadow_models = get_trained_keras_models(cnn_model, shadow_train_data, shadow_test_data, NUM_SHADOW)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNeP0HUEvr9j"
      },
      "source": [
        "attack_train = get_attack_dataset(shadow_models, shadow_train_data, shadow_test_data, NUM_SHADOW, TEST_SIZE)\n",
        "#attack_train=get_attack_data(shadow_models,shadow_train_data,shadow_test_data,5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFqf9yU0yA6v"
      },
      "source": [
        "#calculate attack accuracy against non-private model\n",
        "#ob_model=obj_model(1000)\n",
        "xtrain_agg=x_train[0:n]\n",
        "ytrain_agg=y_train[0:n]\n",
        "#ytrain_agg=ytrain_agg.to_numpy()\n",
        "#ytrain_pred=model_np.predict_proba(xtrain_agg)\n",
        "xtest_agg=x_test[0:n]\n",
        "ytest_agg=y_test[0:n]\n",
        "#ytest_agg=ytest_agg.to_numpy()\n",
        "#ytest_pred=model_np.predict_proba(xtest_agg)\n",
        "target_train_agg = (xtrain_agg,ytrain_agg)\n",
        "target_test_agg = (xtest_agg,ytest_agg)\n",
        "target_train_data, target_test_data = sample_data(target_train_agg, target_test_agg, NUM_TARGET)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBNjYEQR1GHi"
      },
      "source": [
        "#non-private model\n",
        "target_model = get_trained_keras_models(cnn_model, target_train_data, target_test_data, 1)\n",
        "attack_test = get_attack_dataset(target_model, target_train_data, target_test_data, 1, TEST_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOvOC01R83Hc"
      },
      "source": [
        "#calculate the prediction\n",
        "from sklearn.metrics import accuracy_score\n",
        "ypred=target_model[0].predict_classes(xtrain_agg)\n",
        "accuracy_score(label(ytrain_agg), ypred)\n",
        "#print(eps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RVMNqKM7nZm"
      },
      "source": [
        "##attack classifier:RF\n",
        "def get_trained_RF_models(train_data, test_data, num_models):\n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "    (x_train, y_train), (x_test, y_test) = train_data, test_data\n",
        "    models = []\n",
        "    score=np.zeros(num_models)\n",
        "    #RF=RandomForestClassifier(random_state=0)\n",
        "    for i in range(num_models):\n",
        "        print('Training RF model : ', i)\n",
        "        models.append(RandomForestClassifier(random_state=0))\n",
        "        models[i].fit(x_train[i], y_train[i])\n",
        "        score[i] = models[i].score(x_test[i],y_test[i])\n",
        "        print('Random Forest model ', i, 'score : ',score)\n",
        "    return models, score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJhx8jinYiGm"
      },
      "source": [
        "def get_trained_svm_models(train_data, test_data, num_models):\n",
        "    from sklearn import svm\n",
        "    (x_train, y_train), (x_test, y_test) = train_data, test_data\n",
        "    models = []\n",
        "    k=0\n",
        "    for i in range(num_models):\n",
        "        print('Training svm model : ', i)\n",
        "        models.append(svm.SVC(gamma='scale',kernel='linear',verbose=VERBOSE))\n",
        "        models[i].fit(x_train[i], y_train[i])\n",
        "        score = models[i].score(x_test[i],y_test[i])\n",
        "        print('SVM model ', i, 'score : ',score)\n",
        "        k=k+1\n",
        "        print('this is executable',k)\n",
        "    return models,score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1f_ACMh-JK0A"
      },
      "source": [
        "#generate the report #svm model\n",
        "def get_score_svm_models(models, test_data):\n",
        "    from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "    from sklearn.metrics import average_precision_score\n",
        "    (x_test, y_true) = test_data\n",
        "    acc_scores = []\n",
        "    pre_scores = []\n",
        "    rec_scores = []\n",
        "    for i in range(len(models)):\n",
        "        y_pred = models[i].predict(x_test[i])\n",
        "        # _LOG_PRINT(y_pred)\n",
        "        acc_scores.append(accuracy_score(y_true[i], y_pred))\n",
        "        pre_scores.append(average_precision_score(y_true[i], y_pred))\n",
        "        rec_scores.append(recall_score(y_true[i], y_pred))\n",
        "    return np.average(acc_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-y-UPnJx7sbH"
      },
      "source": [
        "attack_model,scores = get_trained_svm_models(attack_train,attack_test, 7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ME_M9VDPNFS3"
      },
      "source": [
        "def calc_labelAcc(Model,xtest,ytest):\n",
        "  from sklearn.metrics import accuracy_score\n",
        "  ypred=Model.predict_classes(xtest)\n",
        "  acc=accuracy_score(label(ytest), ypred)\n",
        "  return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h16DJke_OFhV"
      },
      "source": [
        "def lk(tn,tp,fn,fp):\n",
        "  tpr=(tp/(tp+fn))\n",
        "  fpr=(fp/(fp+tn))\n",
        "  return tpr-fpr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sclt9tIHVN6N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f540195-6b32-49d2-f79e-17656656f751"
      },
      "source": [
        "#privacy leakage non-private model\n",
        "tn1,tp1,fn1,fp1=get_leakage(attack_model, attack_test)\n",
        "lk(tn1,tp1,fn1,fp1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.028240000000000043"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8YGyQIv0ZvC"
      },
      "source": [
        "Privacy Leakage: Input Perturbation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1V0W4n1g0c9u"
      },
      "source": [
        "def Input_pert_train(epi,xtrain,ytrain,xtest,ytest):\n",
        "  delta=.00001\n",
        "  c=1\n",
        "  L=250\n",
        "  E=100\n",
        "  n=25000\n",
        "  T=steps(L,E,n)\n",
        "  print(T)\n",
        "  G=3*np.log(n)\n",
        "  z1=c*(G**2)*T*np.log(1/delta)\n",
        "  z2=n*(n-1)*(epi**2)\n",
        "  sigma=z1/z2\n",
        "  nsi=np.random.normal(loc=0.0, scale=sigma)\n",
        "  #print(nsi)\n",
        "  X_in=xtrain+nsi\n",
        "  target_train_aggin = (X_in,ytrain)\n",
        "  target_test_aggin = (xtest,ytest)\n",
        "  target_train_datain, target_test_datain = sample_data(target_train_aggin, target_test_aggin, 1)\n",
        "  return X_in,target_train_datain,target_test_datain"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKqrE3fi0tlN"
      },
      "source": [
        "ep=[0.01, 0.1,1,10,100,1000,10000]\n",
        "p=len(ep)\n",
        "lin=np.zeros(p)\n",
        "tn0=np.zeros(p)\n",
        "tp0=np.zeros(p)\n",
        "fn0=np.zeros(p)\n",
        "fp0=np.zeros(p)\n",
        "\n",
        "for i in range(len(ep)):\n",
        "     target_model=[]\n",
        "     X_in,target_train_datain,target_test_datain=Input_pert_train(ep[i],xtrain,ytrain,xtest,ytest)\n",
        "     in_model=Model(X_in,ytrain,xtest,ytest)\n",
        "     target_model.append(in_model)\n",
        "     attack_test_agg=get_attack_dataset(target_model, target_train_datain, target_test_datain, 1, TEST_SIZE)\n",
        "     tn0[i],tp0[i],fn0[i],fp0[i]=get_leakage(attack_model, attack_test_agg)\n",
        "     lin[i]=max(0,lk(tn0[i],tp0[i],fn0[i],fp0[i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vKBTCzO0ySt"
      },
      "source": [
        "print(lin)\n",
        "print(tp0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qj4Prgq0MK2o"
      },
      "source": [
        "Test on DP-SGD models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAVTERFvVfIJ"
      },
      "source": [
        "Calculate attack accuracy/Leakage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuS_DCfCeMrw"
      },
      "source": [
        "p=len(ep)\n",
        "lgd=np.zeros(p)\n",
        "tn2=np.zeros(p)\n",
        "tp2=np.zeros(p)\n",
        "fn2=np.zeros(p)\n",
        "fp2=np.zeros(p)\n",
        "for i in range(len(ep)):\n",
        "     target_model=[]\n",
        "     modelSGD=globals()['model_sgd%s' % i]\n",
        "     target_model.append(modelSGD)\n",
        "     attack_test_agg=get_attack_dataset(target_model, target_train_data, target_test_data, 1, TEST_SIZE)\n",
        "     tn2[i],tp2[i],fn2[i],fp2[i]=get_leakage(attack_model, attack_test_agg)\n",
        "     lgd[i]=lk(tn2[i],tp2[i],fn2[i],fp2[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYgYwtYBhqw_"
      },
      "source": [
        "#calc_labelAcc(model_c0,xtest_agg,ytest_agg)*100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uqqd8kgBhlEb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce993bcd-d292-42d1-c495-d21d331b1219"
      },
      "source": [
        "print(lgd)\n",
        "print(tp2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.0022   0.00204 -0.00064  0.00692  0.00412 -0.0088   0.00164]\n",
            "[10961.  8011.  9596. 11952. 12071.  9034. 12584.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MoIkl4YbiFF"
      },
      "source": [
        "[ 0.0022   0.00204 -0.00064  0.00692  0.00412 -0.0088   0.00164]\n",
        "[10961.  8011.  9596. 11952. 12071.  9034. 12584.]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQEXOL-Wbq8X"
      },
      "source": [
        "PATE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSGmHwT0blNF"
      },
      "source": [
        "def PATE(ep,xtest1,ytest1):\n",
        "            sum_t=np.zeros([len(ytest1),7])\n",
        "            for k1 in range(100):\n",
        "               sum=PATE_sum(xtest1,ytest1)\n",
        "               sum_s=np.zeros(sum.shape)\n",
        "               sum_n=sum\n",
        "               sum_s=sum_n+np.random.laplace(loc=0.0, scale=1/ep)\n",
        "               sum_t=sum_s+sum_t\n",
        "\n",
        "            return sum_t/100\n",
        "          \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6rrnxFhbvdT"
      },
      "source": [
        "def PATE_sum(xtest1,ytest1):\n",
        "  sum=np.zeros([len(ytest1),7])\n",
        "  for iter in range(M):\n",
        "    New_model=globals()['model_pate%s' % iter]\n",
        "    y=np.zeros([len(ytest1),7])\n",
        "    y=New_model.predict_proba(xtest1)\n",
        "  #print(y.shape)\n",
        "    y=vote(y)\n",
        "    sum=y+sum\n",
        "  return sum"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaulwvRKb3_Y"
      },
      "source": [
        "p=len(ep)\n",
        "lpate=np.zeros(p)\n",
        "tn4=np.zeros(p)\n",
        "tp4=np.zeros(p)\n",
        "fn4=np.zeros(p)\n",
        "fp4=np.zeros(p)\n",
        "for i in range(len(ep)):\n",
        "     ytrain_pred=PATE(ep[i],xtrain_agg,ytrain_agg)\n",
        "     ytest_pred=PATE(ep[i],xtest_agg,ytest_agg)\n",
        "     attack_test_agg=get_attack_dataset_combined(xtrain_agg,xtest_agg,ytrain_pred,ytrain_agg,ytest_agg,ytest_pred)\n",
        "     print(\"complete\")\n",
        "     tn4[i],tp4[i],fn4[i],fp4[i]=get_leakage(attack_model, attack_test_agg)\n",
        "     lpate[i]=lk(tn4[i],tp4[i],fn4[i],fp4[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uibTR6A_p6it"
      },
      "source": [
        "print(lpate)\n",
        "print(tp4)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "if6d11T5oofv"
      },
      "source": [
        "[0.      0.00028 0.00028 0.00028 0.00028 0.00028 0.00028]\n",
        "[10000. 14733. 14733. 14733. 14733. 14733. 14733.]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OS28bLjiiA6s"
      },
      "source": [
        "plot the graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBmYz7HViCh9"
      },
      "source": [
        "import pandas as pd \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns #grafikleştirme için\n",
        "import matplotlib.pyplot as plt \n",
        "from google.colab import files\n",
        "test1 = plt.figure()\n",
        "#plt.semilogx(ep,lobj,color=\"black\",marker='3',label='Objective Perturbation',linewidth=1.5)\n",
        "plt.semilogx(ep,lin,color=\"black\",marker='^',label='Input Perturbation',linewidth=1.5)\n",
        "plt.semilogx(ep,lgd,color=\"red\",marker='*',label='Gradient Perturbation',linewidth=1.5)\n",
        "#plt.semilogx(ep,lout,color=\"black\",marker='+',linestyle=\"--\",label='Output Perturbation',linewidth=1.5)\n",
        "plt.semilogx(ep,lpate,color=\"brown\",marker='o',label='Prediction Perturbation',linewidth=1.5)\n",
        "#plt.semilogx(ep,non_p-acc_in,color=\"orange\",marker='.',linestyle=\"--\",label='Input',linewidth=1.5)\n",
        "#plt.plot(ep,non_p,color=\"red\",marker='*',linestyle=\"--\",label='Non-Private Model',linewidth=2)\n",
        "plt.legend(loc=1,fontsize=12)\n",
        "plt.xlabel(\"Privacy Budget($\\epsilon$)\",fontsize=13)\n",
        "plt.ylabel(\"Privacy Leakage\",fontsize=15)\n",
        "#plt.xscale('symlog', linthreshy=0.1)\n",
        "#plt.ylim([-.1,1])\n",
        "plt.xticks(size = 10)\n",
        "plt.yticks(size = 8)\n",
        "plt.ylim([-.03,.25])\n",
        "#y.set_color(\"black\")\n",
        "plt.rcParams[\"axes.edgecolor\"] = \"black\"\n",
        "plt.rcParams[\"axes.linewidth\"] = 1\n",
        "plt.rcParams['axes.facecolor'] = 'white'\n",
        "#test1.set_facecolor('white')\n",
        "test1.show()\n",
        "test1.savefig('DNN_perturb_leakage_loan.pdf')\n",
        "files.download('DNN_perturb_leakage_loan.pdf')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxN1P1T_j5tX"
      },
      "source": [
        "import pandas as pd \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns #grafikleştirme için\n",
        "import matplotlib.pyplot as plt \n",
        "from google.colab import files\n",
        "test1 = plt.figure()\n",
        "#plt.semilogx(ep,tp1,color=\"black\",marker='D',label='Objective Perturbation',linewidth=1.5)\n",
        "plt.semilogx(ep,tp0,color=\"black\",marker='^',label='Input Perturbation',linewidth=1.5)\n",
        "plt.semilogx(ep,tp2,color=\"red\",marker='*',label='Gradient Perturbation',linewidth=1.5)\n",
        "#plt.semilogx(ep,tp3,color=\"black\",marker='<',label='Output Perturbation',linewidth=1.5)\n",
        "plt.semilogx(ep,tp4,color=\"brown\",marker='o',label='Prediction Perturbation',linewidth=1.5)\n",
        "#plt.semilogx(ep,non_p-acc_in,color=\"orange\",marker='.',linestyle=\"--\",label='Input',linewidth=1.5)\n",
        "#plt.plot(ep,non_p,color=\"red\",marker='*',linestyle=\"--\",label='Non-Private Model',linewidth=2)\n",
        "plt.legend(loc=1,fontsize=10)\n",
        "plt.xlabel(\"Privacy Budget($\\epsilon$)\",fontsize=13)\n",
        "plt.ylabel(\"True Revealed Data (True Positive)\",fontsize=12)\n",
        "#plt.xscale('symlog', linthreshy=0.1)\n",
        "#plt.ylim([-.1,1])\n",
        "plt.xticks(size = 10)\n",
        "plt.yticks(size = 8)\n",
        "plt.ylim([0,25000])\n",
        "#y.set_color(\"black\")\n",
        "plt.rcParams[\"axes.edgecolor\"] = \"black\"\n",
        "plt.rcParams[\"axes.linewidth\"] = 1\n",
        "plt.rcParams['axes.facecolor'] = 'white'\n",
        "#test1.set_facecolor('white')\n",
        "test1.show()\n",
        "test1.savefig('DNN_perturb_tp_loan.pdf')\n",
        "files.download('DNN_perturb_tp_loan.pdf')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}