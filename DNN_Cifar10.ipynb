{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "DNN_Cifar10.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sprivacy/DP-Utility-in-ML/blob/main/DNN_Cifar10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gl2icL_uqhTu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ac24b4d-99f0-4beb-84c1-32a8546c304f"
      },
      "source": [
        "#Mount Data on your google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YsJpn5k3rEQG",
        "outputId": "c5a180ee-5e84-41e5-a2f2-20785df1a667"
      },
      "source": [
        "#Neccessary Libraries for DP-SGD\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 1.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "tf.compat.v1.logging.set_verbosity(tf.logging.ERROR)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hn_yiwsfrKZN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f72d3829-9a0d-479a-da83-8acccc6038f9"
      },
      "source": [
        "#importing libraries\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "from matplotlib import pyplot\n",
        "from keras.datasets import cifar10\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.optimizers import SGD\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9r8-WR5rRI8"
      },
      "source": [
        "!pip install tensorflow_privacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Th14hmlfrvJ0"
      },
      "source": [
        "from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy\n",
        "from tensorflow_privacy.privacy.optimizers.dp_optimizer import DPGradientDescentGaussianOptimizer,DPAdamGaussianOptimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlclTCabr0v1"
      },
      "source": [
        "def load_dataset():\n",
        "    (trainX,trainY), (testX,testY)=cifar10.load_data()\n",
        "    trainX=trainX.reshape(50000,32*32*3)\n",
        "    testX=testX.reshape(10000,32*32*3)\n",
        "    x1=trainX[30000:50000]\n",
        "    Xtest=np.vstack((x1,testX)) \n",
        "    y1=trainY[30000:50000]\n",
        "    ytest=np.vstack((y1,testY))\n",
        "    Xtrain=trainX[0:30000]\n",
        "    ytrain=trainY[0:30000]\n",
        "    return Xtrain, ytrain, Xtest, ytest\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeXs6FsUr3s_"
      },
      "source": [
        "def prep_pixels(train, test):\n",
        "\t# convert from integers to floats\n",
        "\ttrain_norm = train.astype('float32')\n",
        "\ttest_norm = test.astype('float32')\n",
        "\t# normalize to range 0-1\n",
        "\ttrain_norm = train_norm / 255.0\n",
        "\ttest_norm = test_norm / 255.0\n",
        "\t# return normalized images\n",
        "\treturn train_norm, test_norm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RW4G7Gcr-D1"
      },
      "source": [
        "# load the pre-shuffled train and test data\n",
        "X_train,y_train,X_test,y_test=load_dataset()\n",
        "X_train,X_test=prep_pixels(X_train,X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWm5jMF3sC7V"
      },
      "source": [
        "x_train=X_train.reshape(X_train.shape[0],len(X_train[0]),1)\n",
        "x_test=X_test.reshape(X_test.shape[0],len(X_test[0]),1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsMJkAmWsYgz"
      },
      "source": [
        "y_train=to_categorical(y_train)\n",
        "y_test=to_categorical(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpAfrl0hyDF9"
      },
      "source": [
        "def label(my_list):\n",
        "  import numpy as np\n",
        "  my_array=np.array(my_list)\n",
        "  p=np.zeros(my_array.shape)\n",
        "  b=my_array.max(-1)\n",
        "  condition = my_array == b[..., np.newaxis]\n",
        "  c = np.where(condition, 1, 0)\n",
        "  final=np.multiply(c, my_array)\n",
        "  #my_sum=np.sum(final,axis=0)\n",
        "  labels=np.argmax(final, axis=1)\n",
        "  return labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5Gk8XV1j_tA"
      },
      "source": [
        "Input Perturbation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2isuaT4kkEBu"
      },
      "source": [
        "n=15000\n",
        "xtrain=x_train[0:n]\n",
        "ytrain=y_train[0:n]\n",
        "xtest=x_test[0:n]\n",
        "ytest=y_test[0:n]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXwTFNd_kGwa"
      },
      "source": [
        "#define the DNN model\n",
        "def Model(xtrain,ytrain,x_test,y_test):\n",
        "  #For non-private Settings\n",
        "  from keras.regularizers import l2\n",
        "  opt = tf.keras.optimizers.Adam(learning_rate=.01)\n",
        "  model_np = tf.keras.Sequential([\n",
        "   #tf.keras.layers.Flatten(),\n",
        "   tf.keras.layers.Dense(64,\n",
        "                           activation='relu',\n",
        "                           input_shape=(32*32*3,1)),\n",
        "   #tf.keras.layers.MaxPool2D(2, 1),\n",
        "   tf.keras.layers.Dense(64,\n",
        "                           activation='relu'),\n",
        "   #tf.keras.layers.MaxPool2D(2, 1),\n",
        "   tf.keras.layers.Flatten(),\n",
        "   #tf.keras.layers.Dense(50,activation='relu'),\n",
        "   #tf.keras.layers.Dense(25, activation='relu'),\n",
        "   tf.keras.layers.Dense(10, activation='softmax')\n",
        "       ])\n",
        "  loss = tf.keras.losses.CategoricalCrossentropy(\n",
        "    from_logits=True, reduction=tf.losses.Reduction.NONE)\n",
        "  model_np.compile(optimizer=opt, loss=loss, metrics=['accuracy'])\n",
        "  model_np.fit(xtrain, ytrain,\n",
        "          epochs=100,\n",
        "          validation_data=(x_test[0:1000], y_test[0:1000]),\n",
        "          batch_size=250)\n",
        "  return model_np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcHIDqb4kOUi"
      },
      "source": [
        "#label accuracy\n",
        "def calc_labelAcc(Model,xtest,ytest):\n",
        "  from sklearn.metrics import accuracy_score\n",
        "  ypred=Model.predict(xtest)\n",
        "  acc=accuracy_score(label(ytest), label(ypred))\n",
        "  return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvzlE45PkTeM"
      },
      "source": [
        "#non-private accuracy\n",
        "model_np=Model(xtrain,ytrain,xtest,ytest)\n",
        "acc_np=calc_labelAcc(model_np,xtest,ytest)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33KITw-ikWIV"
      },
      "source": [
        "#calculate total number of steps, T\n",
        "def steps(L,E,n):\n",
        "  q=L/n\n",
        "  T=E/q\n",
        "  return T\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwlHpw3bkhP8"
      },
      "source": [
        "ep=[0.01, 0.1,1,10,100,1000,10000]\n",
        "ni=len(ep)\n",
        "acc_in=np.zeros(ni)\n",
        "for i in range(ni):\n",
        "  epi=ep[i]\n",
        "  delta=.00001\n",
        "  c=1\n",
        "  L=250\n",
        "  E=100\n",
        "  T=steps(L,E,n)\n",
        "  print(T)\n",
        "  G=3*np.log(n)\n",
        "  z1=c*(G**2)*T*np.log(1/delta)\n",
        "  z2=n*(n-1)*(epi**2)\n",
        "  sigma=z1/z2\n",
        "  nsi=np.random.normal(loc=0.0, scale=sigma)\n",
        "  print(nsi)\n",
        "  X_in=xtrain+nsi\n",
        "  model_in=Model(X_in,ytrain,xtest,ytest)\n",
        "  acc_in[i]=calc_labelAcc(model_in,xtest,ytest)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bz0x9jDbkkHF"
      },
      "source": [
        "#accuracy loss for Input perturbation over different epsilon value\n",
        "util_in=acc_np-acc_in\n",
        "util_in"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTjslDdekBrC"
      },
      "source": [
        "Gradient Perturbation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StzmI47kyF_O"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ReduceLROnPlateau\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Lj08Wc_yJPd"
      },
      "source": [
        "def DP_SGD(noise_multiplier,x_train,y_train,x_test,y_test):\n",
        "   epochs = 100\n",
        "   batch_size = 250\n",
        "   l2_norm_clip = 1.5\n",
        "   #noise_multiplier = .88\n",
        "   num_microbatches = 125\n",
        "   learning_rate = 0.001\n",
        "   x_train_s=x_train\n",
        "   y_train_s=y_train\n",
        "   x_test=x_test\n",
        "   y_test_s=y_test\n",
        "   n=len(y_train)\n",
        "   #noise_multiplier=.5\n",
        "   if batch_size % num_microbatches != 0:\n",
        "     raise ValueError('Batch size should be an integer multiple of the number of microbatches')\n",
        "   \n",
        "   eps=compute_dp_sgd_privacy.compute_dp_sgd_privacy(n=n, batch_size=250, noise_multiplier=noise_multiplier, epochs=epochs, delta=1e-5)\n",
        "   print(eps)\n",
        "\n",
        "   model = tf.keras.Sequential([\n",
        "   tf.keras.layers.Dense(64,\n",
        "                           activation='relu',\n",
        "                           input_shape=(32*32*3,1)),\n",
        "   tf.keras.layers.Dense(64,\n",
        "                           activation='relu'),\n",
        "   #tf.keras.layers.MaxPool2D(2, 1),\n",
        "   tf.keras.layers.Flatten(),\n",
        "   tf.keras.layers.Dense(10, activation='softmax')\n",
        "       ])\n",
        "   print(\"compplete\")\n",
        "   optimizer = DPAdamGaussianOptimizer(\n",
        "            l2_norm_clip=l2_norm_clip,\n",
        "            noise_multiplier=noise_multiplier,\n",
        "            num_microbatches=num_microbatches,\n",
        "            learning_rate=learning_rate)\n",
        "   \n",
        "   loss = tf.keras.losses.CategoricalCrossentropy(\n",
        "    from_logits=True, reduction=tf.losses.Reduction.NONE)\n",
        "   model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "   model.fit(x_train_s, y_train_s,\n",
        "          epochs=epochs,\n",
        "          validation_data=(x_test[0:1000], y_test_s[0:1000]),\n",
        "          batch_size=batch_size)\n",
        "   return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gn6pCyTrySpo"
      },
      "source": [
        "#model=SGD_lg(1,x_train[0:15000],trainY[0:15000],x_test,testY)\n",
        "def calc_labelAcc(Model,xtest,ytest):\n",
        "  from sklearn.metrics import accuracy_score\n",
        "  ypred=Model.predict_classes(xtest)\n",
        "  acc=accuracy_score(label(ytest), ypred)\n",
        "  return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akNznM-4yjjy"
      },
      "source": [
        "###training and testing data\n",
        "n=15000\n",
        "xtrain_agg=x_train[0:n]\n",
        "ytrain_agg=y_train[0:n]\n",
        "#ytrain_agg=ytrain_agg.to_numpy()\n",
        "#ytrain_pred=model_np.predict_proba(xtrain_agg)\n",
        "xtest_agg=x_test[0:n]\n",
        "ytest_agg=y_test[0:n]\n",
        "#ytest_agg=ytest_agg.to_numpy()\n",
        "#ytest_pred=model_np.predict_proba(xtest_agg)\n",
        "target_train_agg = (xtrain_agg,ytrain_agg)\n",
        "target_test_agg = (xtest_agg,ytest_agg)\n",
        "#target_train_data_agg, target_test_data_agg = sample_data(target_train_agg, target_test_agg, NUM_TARGET)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctymL9lFym3N"
      },
      "source": [
        "noise_mul=[370,42,5.27,.955,.435,.262,.1865]  #noise multiplier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMdrWyxLz-2P"
      },
      "source": [
        "#For non-private Settings\n",
        "from keras.regularizers import l2\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=.01)\n",
        "model_np = tf.keras.Sequential([\n",
        "   #tf.keras.layers.Flatten(),\n",
        "   tf.keras.layers.Dense(64,\n",
        "                           activation='relu',\n",
        "                           input_shape=(32*32*3,1)),\n",
        "   #tf.keras.layers.MaxPool2D(2, 1),\n",
        "   tf.keras.layers.Dense(64,\n",
        "                           activation='relu'),\n",
        "   #tf.keras.layers.MaxPool2D(2, 1),\n",
        "   tf.keras.layers.Flatten(),\n",
        "   #tf.keras.layers.Dense(50,activation='relu'),\n",
        "   #tf.keras.layers.Dense(25, activation='relu'),\n",
        "   tf.keras.layers.Dense(10, activation='softmax')\n",
        "       ])\n",
        "loss = tf.keras.losses.CategoricalCrossentropy(\n",
        "    from_logits=True, reduction=tf.losses.Reduction.NONE)\n",
        "model_np.compile(optimizer=opt, loss=loss, metrics=['accuracy'])\n",
        "model_np.fit(xtrain_agg, ytrain_agg,\n",
        "          epochs=100,\n",
        "          validation_data=(x_test[0:1000], y_test[0:1000]),\n",
        "          batch_size=250)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmsktErbIMaS"
      },
      "source": [
        "np_sgd=calc_labelAcc(model_np,xtrain_agg,ytrain_agg)\n",
        "np_sgd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6whx5zP_y-KY"
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "acc_sgd= np.zeros(len(noise_mul))\n",
        "model_index =0\n",
        "for i in range(7):\n",
        "  model=DP_SGD(noise_mul[i],xtrain_agg,ytrain_agg,xtest_agg,ytest_agg)\n",
        "  acc_sgd[i]=calc_labelAcc(model,xtest_agg,ytest_agg)\n",
        "  globals()['model_sgd%s' % i]=model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dFHb9JeH-u6"
      },
      "source": [
        "utility_sgd=np_sgd-acc_sgd\n",
        "print(utility_sgd)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlOEE0nGJV0f"
      },
      "source": [
        "PATE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYJ34ZH7JYEe"
      },
      "source": [
        "def build_cnn_model():\n",
        "    import keras\n",
        "    from keras.models import Sequential\n",
        "    from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "    num_class=10\n",
        "    # build the model\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64, activation='relu', input_shape=(len(x_train[0]),1)))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Flatten())\n",
        "    #model.add(Dense(50, activation='relu'))\n",
        "    #model.add(Dense(64, activation='tanh'))\n",
        "    if num_class==1:\n",
        "        model.add(Dense(num_class, activation='sigmoid'))    \n",
        "    else:\n",
        "        model.add(Dense(num_class, activation='softmax')) \n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8IGnK_AJq5V",
        "outputId": "e6faf8b9-5194-4938-c9c6-560b83ec640d"
      },
      "source": [
        "#split datasets into multiple teacher\n",
        "#Divide the images into 5 \n",
        "# split x_train to 10 disjoint datasets\n",
        "#store each dataset variable name is the list Xtrain\n",
        "import numpy as np\n",
        "M=40\n",
        "j=0\n",
        "k=len(xtrain_agg)/M\n",
        "#print(x_train)\n",
        "\n",
        "for x in range(0,M):\n",
        "             globals()['x_train_split%s' % x]=xtrain_agg[int(j):int(k+j)]\n",
        "             globals()['y_train_split%s' % x]=ytrain_agg[int(j):int(k+j)]\n",
        "             j=k+j\n",
        "             #print(j)\n",
        "             \n",
        "print(x_train_split29.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(375, 3072, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtg7UaztJs5z"
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "model_index =0\n",
        "for iter in range(M):\n",
        "  model=build_cnn_model()\n",
        "  model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  model.fit(globals()['x_train_split%s' % iter], globals()['y_train_split%s' % iter], batch_size=250, epochs=100, verbose=1, shuffle=True)\n",
        "  #save the models\n",
        "  globals()['model_pate%s' % iter]=model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-4y5HHjKUxo"
      },
      "source": [
        "#function for vote counting\n",
        "def vote(my_list):\n",
        "  import numpy as np\n",
        "  my_array=np.array(my_list)\n",
        "  p=np.zeros(my_array.shape)\n",
        "  b=my_array.max(-1)\n",
        "  condition = my_array == b[..., np.newaxis]\n",
        "  c = np.where(condition, 1, 0)\n",
        "  final=np.multiply(c, my_array)\n",
        "  #my_sum=np.sum(final,axis=0)\n",
        "  labels=np.argmax(final, axis=1)\n",
        "  return c"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4JQNR_6KYf9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdddd05c-ca9e-4545-92f6-78e08880e22a"
      },
      "source": [
        "sum=np.zeros([len(y_test),10])\n",
        "for i in range(M):\n",
        "  New_model=globals()['model_pate%s' % i]\n",
        "  y=np.zeros(y_test.shape)\n",
        "  y=New_model.predict_proba(x_test)\n",
        "  y=vote(y)\n",
        "  sum=y+sum\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:425: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
            "  warnings.warn('`model.predict_proba()` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmEiSy2BKcX0"
      },
      "source": [
        "#create the labels\n",
        "from sklearn.metrics import accuracy_score\n",
        "lab=label(sum)\n",
        "#clear accuracy\n",
        "y_true=label(y_test)\n",
        "np_pate=accuracy_score(y_true,lab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_AHJ8MSKgnO"
      },
      "source": [
        "ep=[.01,.1,1,10,100,1000,10000]\n",
        "client_acc=np.zeros(len(ep))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoSdBMgAKj6P"
      },
      "source": [
        "#add noise \n",
        "def add_noise_sum(noise,sum,experiment,ypred):\n",
        "  sum_s=sum\n",
        "  predt=np.zeros(experiment)\n",
        "  for i1 in range(experiment):\n",
        "    sum_s=np.zeros(sum.shape)\n",
        "    sum_f=sum+np.random.laplace(loc=0.0, scale=1/noise)\n",
        "    sum2=label(sum_f)\n",
        "    predt[i1]=accuracy_score(ypred,sum2)\n",
        "    #print(i)\n",
        "  pred=np.average(predt)\n",
        "  #print(predt)\n",
        "  #print(pred)\n",
        "  return pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFRThV_uKnsa",
        "outputId": "80529747-27de-4c32-f4bb-348feecda53d"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import average_precision_score\n",
        "#Noise after Aggregation Method\n",
        "for i in range(0,len(ep)):\n",
        "                          sum_s=np.zeros(sum.shape)\n",
        "                          sum_n=sum\n",
        "                          #sum_s=sum_n+np.random.laplace(loc=0.0, scale=1/noise[i])\n",
        "                          predF=add_noise_sum(ep[i],sum_n,100,y_true)\n",
        "                          client_acc[i]=predF\n",
        "print(client_acc)\n",
        "print(ep)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.23826467 0.35644533 0.39313333 0.39313333 0.39313333 0.39313333\n",
            " 0.39313333]\n",
            "[0.01, 0.1, 1, 10, 100, 1000, 10000]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdPMZE_KKsa7"
      },
      "source": [
        "utility_pate=np_pate-client_acc\n",
        "print(utility_pate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-IyOOjXLW94"
      },
      "source": [
        "Figure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8g7-o35LYy6"
      },
      "source": [
        "import pandas as pd \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns #grafikleştirme için\n",
        "import matplotlib.pyplot as plt \n",
        "from google.colab import files\n",
        "test1 = plt.figure()\n",
        "#plt.semilogx(ep,non_p-acc_obj,color=\"black\",marker='3',label='Objective Perturbation',linewidth=1.5)\n",
        "plt.semilogx(ep,util_in,color=\"black\",marker='^',label='Input Perturbation',linewidth=1.5)\n",
        "plt.semilogx(ep,utility_sgd,color=\"red\",marker='*',label='Gradient Perturbation',linewidth=1.5)\n",
        "#plt.semilogx(ep,np_out-acc_out,color=\"black\",marker='+',linestyle=\"--\",label='Output Perturbation',linewidth=1.5)\n",
        "plt.semilogx(ep,utility_pate,color=\"brown\",marker='o',label='Prediction Perturbation',linewidth=1.5)\n",
        "#plt.semilogx(ep,non_p-acc_in,color=\"orange\",marker='.',linestyle=\"--\",label='Input',linewidth=1.5)\n",
        "#plt.plot(ep,non_p,color=\"red\",marker='*',linestyle=\"--\",label='Non-Private Model',linewidth=2)\n",
        "plt.legend(loc=1,fontsize=12)\n",
        "plt.xlabel(\"Privacy Budget($\\epsilon$)\",fontsize=13)\n",
        "plt.ylabel(\"Utility Loss\",fontsize=15)\n",
        "#plt.xscale('symlog', linthreshy=0.1)\n",
        "#plt.ylim([-.1,1])\n",
        "plt.xticks(size = 10)\n",
        "plt.yticks(size = 8)\n",
        "plt.ylim([-.05,1])\n",
        "#y.set_color(\"black\")\n",
        "plt.rcParams[\"axes.edgecolor\"] = \"black\"\n",
        "plt.rcParams[\"axes.linewidth\"] = 1\n",
        "plt.rcParams['axes.facecolor'] = 'white'\n",
        "#test1.set_facecolor('white')\n",
        "test1.show()\n",
        "test1.savefig('DNN_perturb_acc_cifar.pdf')\n",
        "files.download('DNN_perturb_acc_cifar.pdf')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raujQjCjSqqX"
      },
      "source": [
        "Attack"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3C-g4UjHSsDu"
      },
      "source": [
        "#Assign necessary variables for attacking the model\n",
        "import argparse\n",
        "import csv\n",
        "import numpy as np\n",
        "from sklearn.utils import resample, shuffle\n",
        "\n",
        "import keras\n",
        "import numpy as np\n",
        "from sklearn.utils import resample\n",
        "\n",
        "LEARNING_RATE = 0.001\n",
        "#EPOCH = 100\n",
        "EPOCH = 100\n",
        "DATA_SIZE = 30000\n",
        "TRAINING_SIZE = 15000\n",
        "TEST_SIZE = 15000\n",
        "NUM_TARGET = 1\n",
        "#NUM_SHADOW = 100\n",
        "NUM_SHADOW = 10\n",
        "IN = 1\n",
        "OUT = 0\n",
        "VERBOSE = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuQp3qiQS1n2"
      },
      "source": [
        "#call required libraries\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.utils import to_categorical \n",
        "import sys\n",
        "from matplotlib import pyplot\n",
        "from keras.datasets import cifar10\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.optimizers import SGD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTFfzB8YS5B8"
      },
      "source": [
        "#Define model configuaration\n",
        "# Model configuration\n",
        "batch_size = 250\n",
        "#img_width, img_height, img_num_channels = 32, 32, 3\n",
        "#loss_function = sparse_categorical_crossentropy\n",
        "no_classes = 10\n",
        "no_epochs = 250\n",
        "optimizer = Adam()\n",
        "validation_split = 0.2\n",
        "verbosity = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoAhcUbVS99z"
      },
      "source": [
        "#data sampling\n",
        "def sample_data(train_data,test_data,num_sets):\n",
        "    (x_train, y_train), (x_test, y_test) = train_data, test_data\n",
        "    new_x_train, new_y_train = [], []\n",
        "    new_x_test, new_y_test = [], []\n",
        "    for i in range(num_sets):\n",
        "        x_temp, y_temp = resample(x_train, y_train, n_samples=len(y_train), random_state=0)\n",
        "        new_x_train.append(x_temp)\n",
        "        new_y_train.append(y_temp)\n",
        "        x_temp, y_temp = resample(x_test, y_test, n_samples=len(y_test), random_state=0)\n",
        "        new_x_test.append(x_temp)\n",
        "        new_y_test.append(y_temp)\n",
        "    return (new_x_train, new_y_train), (new_x_test, new_y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abNtrs6ATBOe"
      },
      "source": [
        "def get_trained_keras_models(keras_model, train_data, test_data, num_models):\n",
        "    (x_train, y_train), (x_test, y_test) = train_data, test_data\n",
        "    models = []\n",
        "    for i in range(num_models):\n",
        "        models.append(keras.models.clone_model(keras_model))\n",
        "        models[i].compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "        models[i].fit(x_train[i], y_train[i], batch_size=250, epochs=EPOCH, verbose=VERBOSE, shuffle=True)\n",
        "        score = models[i].evaluate(x_test[i], y_test[i], verbose=VERBOSE)\n",
        "        print('\\n', 'Model ', i, ' test accuracy:', score[1])\n",
        "    return models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfnsM9HyTHsJ"
      },
      "source": [
        "#collect the attack dataset from shadow models\n",
        "def get_attack_dataset(models, train_data, test_data, num_models, data_size):\n",
        "    # generate dataset for the attack model\n",
        "    (x_train, y_train), (x_test, y_test) = train_data, test_data\n",
        "    num_classes = 10#len(y_train[0][0])\n",
        "    x_data, y_data = [[] for i in range(num_classes)], [[] for i in range(num_classes)]\n",
        "    for i in range(num_models):\n",
        "        # IN data\n",
        "        x_temp, y_temp = resample(x_train[i], y_train[i], n_samples=data_size, random_state=0)\n",
        "        for j in range(data_size):\n",
        "            y_idx = np.argmax(y_temp[j])\n",
        "            x_data[y_idx].append(models[i].predict(x_temp[j:j+1])[0])\n",
        "            #print(y_idx)\n",
        "            y_data[y_idx].append(IN)\n",
        "            print(\"starts1\",j)\n",
        "        # OUT data\n",
        "        x_temp, y_temp = resample(x_test[i], y_test[i], n_samples=data_size, random_state=0)\n",
        "        for j in range(data_size):\n",
        "            y_idx = np.argmax(y_temp[j])\n",
        "            p=models[i].predict(x_temp[j:j+1])[0]\n",
        "            x_data[y_idx].append(p)\n",
        "            y_data[y_idx].append(OUT)\n",
        "            print(\"starts2\",j)\n",
        "    return x_data, y_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biQyrWQ7TM-5"
      },
      "source": [
        "#collect the attack dataset from target models\n",
        "def get_target_dataset(models, train_data, test_data, num_models, data_size):\n",
        "    # generate dataset for the attack model\n",
        "    (x_train, y_train), (x_test, y_test) = train_data, test_data\n",
        "    num_classes = 10#len(y_train[0][0])\n",
        "    x_data, y_data = [[] for i in range(num_classes)], [[] for i in range(num_classes)]\n",
        "    for i in range(num_models):\n",
        "        # IN data\n",
        "        x_temp, y_temp = resample(x_train[i], y_train[i], n_samples=data_size, random_state=0)\n",
        "        for j in range(data_size):\n",
        "            y_idx = np.argmax(y_temp[j])\n",
        "            x_data[y_idx].append(models[i].predict(x_temp[j:j+1])[0])\n",
        "            #print(y_idx)\n",
        "            y_data[y_idx].append(IN)\n",
        "            print(\"starts1\",j)\n",
        "        # OUT data\n",
        "        x_temp, y_temp = resample(x_test[i], y_test[i], n_samples=data_size, random_state=0)\n",
        "        for j in range(data_size):\n",
        "            y_idx = np.argmax(y_temp[j])\n",
        "            p=models[i].predict(x_temp[j:j+1])[0]\n",
        "            x_data[y_idx].append(p)\n",
        "            y_data[y_idx].append(OUT)\n",
        "            print(\"starts2\",j)\n",
        "    return x_data, y_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmXnmZhOTRpk"
      },
      "source": [
        "#Get the prediction vectors from the DP model\n",
        "def get_attack_dataset_combined(x_train, x_test, train_pred, y_train, y_test, test_pred):\n",
        "    # generate dataset for the attack model\n",
        "    #(x_train, y_train), (x_test, y_test) = train_data, test_data\n",
        "    data_size=len(y_train)\n",
        "    num_class = 10\n",
        "    x_data, y_data = [[] for _ in range(num_class)], [[] for _ in range(num_class)]\n",
        "    #for i in range(num_models):\n",
        "        # IN data\n",
        "    #x_temp, y_temp = resample(x_train, y_train, n_samples=data_size, random_state=0)\n",
        "    for j in range(data_size):\n",
        "            y_idx = np.argmax(y_train[j])\n",
        "            x_data[y_idx].append(train_pred[j])\n",
        "            #print(train_pred[j])\n",
        "            #x_data[y_idx].append(models.predict(x_temp[j:j+1])[0])\n",
        "            y_data[y_idx].append(IN)\n",
        "        # OUT data\n",
        "    #x_temp, y_temp = resample(x_test, y_test, n_samples=data_size, random_state=0)\n",
        "    for j in range(data_size):\n",
        "            y_idx = np.argmax(y_test[j])\n",
        "            #x_data[y_idx].append(models.predict(x_temp[j:j+1])[0])\n",
        "            x_data[y_idx].append(test_pred[j])\n",
        "            y_data[y_idx].append(OUT)\n",
        "    return x_data, y_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhCMwpdSTWPn"
      },
      "source": [
        "def build_cnn_model():\n",
        "    import keras\n",
        "    from keras.models import Sequential\n",
        "    from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "    num_class=10\n",
        "    # build the model\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64, activation='relu', input_shape=(len(x_train[0]),1)))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Flatten())\n",
        "    #model.add(Dense(50, activation='relu'))\n",
        "    #model.add(Dense(64, activation='tanh'))\n",
        "    if num_class==1:\n",
        "        model.add(Dense(num_class, activation='sigmoid'))    \n",
        "    else:\n",
        "        model.add(Dense(num_class, activation='softmax')) \n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XV9wWCPaTbfg"
      },
      "source": [
        "#generate the report\n",
        "def get_leakage(models, test_data):\n",
        "    from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "    from sklearn.metrics import average_precision_score\n",
        "    from sklearn import metrics\n",
        "    from sklearn.metrics import confusion_matrix\n",
        "    (x_test, y_true) = test_data\n",
        "    acc_scores = []\n",
        "    pre_scores = []\n",
        "    rec_scores = []\n",
        "    fp=np.zeros(len(models))\n",
        "    tp=np.zeros(len(models))\n",
        "    tn=np.zeros(len(models))\n",
        "    fn=np.zeros(len(models))\n",
        "    for i in range(len(models)):\n",
        "        y_pred = models[i].predict(x_test[i])\n",
        "        # _LOG_PRINT(y_pred)\n",
        "        acc_scores.append(accuracy_score(y_true[i], y_pred))\n",
        "        pre_scores.append(average_precision_score(y_true[i], y_pred))\n",
        "        rec_scores.append(recall_score(y_true[i], y_pred))\n",
        "        tn[i], fp[i], fn[i], tp[i] = confusion_matrix(y_true[i], y_pred).ravel()\n",
        "    return np.sum(tn),np.sum(tp),np.sum(fn),np.sum(fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4XCEY0akjk9"
      },
      "source": [
        "#generate the report\n",
        "def get_lkg(models, test_data):\n",
        "    from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "    from sklearn.metrics import average_precision_score\n",
        "    from sklearn import metrics\n",
        "    #from sklearn.metrics import confusion_matrix\n",
        "    (x_test, y_true) = test_data\n",
        "    acc_scores = []\n",
        "    pre_scores = []\n",
        "    rec_scores = []\n",
        "    #fpr=np.zeros(len(models))\n",
        "    #tpr=np.zeros(len(models))\n",
        "    #tn=np.zeros(len(models))\n",
        "    #fn=np.zeros(len(models))\n",
        "    lkg=np.zeros(len(models))\n",
        "    for i in range(len(models)):\n",
        "        y_pred = models[i].predict(x_test[i])\n",
        "        # _LOG_PRINT(y_pred)\n",
        "        acc_scores.append(accuracy_score(y_true[i], y_pred))\n",
        "        pre_scores.append(average_precision_score(y_true[i], y_pred))\n",
        "        rec_scores.append(recall_score(y_true[i], y_pred))\n",
        "        fpr1, tpr1,t = metrics.roc_curve(y_true[i], y_pred,pos_label=1)\n",
        "        lkg[i]=np.max(tpr1-fpr1)\n",
        "        #print(fpr)\n",
        "        #print(tpr)\n",
        "    return lkg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWfoM1FdTfdQ"
      },
      "source": [
        "n=15000\n",
        "shadow_train = (x_train[n:n*2],y_train[n:n*2])\n",
        "shadow_test = (x_test[n:n*2],y_test[n:n*2])\n",
        "shadow_train_data, shadow_test_data = sample_data(shadow_train, shadow_test, NUM_SHADOW)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXCix6xATkSY"
      },
      "source": [
        "cnn_model=build_cnn_model()\n",
        "# compile the shadow models\n",
        "shadow_models = get_trained_keras_models(cnn_model, shadow_train_data, shadow_test_data, NUM_SHADOW)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2u2OkyA0WE7s"
      },
      "source": [
        "attack_train = get_attack_dataset(shadow_models, shadow_train_data, shadow_test_data, NUM_SHADOW, TEST_SIZE)\n",
        "#attack_train=get_attack_data(shadow_models,shadow_train_data,shadow_test_data,5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sjc5TYSXWI_7"
      },
      "source": [
        "#calculate attack accuracy against non-private model\n",
        "#ob_model=obj_model(1000)\n",
        "xtrain_agg=x_train[0:n]\n",
        "ytrain_agg=y_train[0:n]\n",
        "#ytrain_agg=ytrain_agg.to_numpy()\n",
        "#ytrain_pred=model_np.predict_proba(xtrain_agg)\n",
        "xtest_agg=x_test[0:n]\n",
        "ytest_agg=y_test[0:n]\n",
        "#ytest_agg=ytest_agg.to_numpy()\n",
        "#ytest_pred=model_np.predict_proba(xtest_agg)\n",
        "target_train_agg = (xtrain_agg,ytrain_agg)\n",
        "target_test_agg = (xtest_agg,ytest_agg)\n",
        "target_train_data, target_test_data = sample_data(target_train_agg, target_test_agg, NUM_TARGET)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_f0dngFpWL2K"
      },
      "source": [
        "#non-private model\n",
        "target_model = get_trained_keras_models(cnn_model, target_train_data, target_test_data, 1)\n",
        "attack_test = get_attack_dataset(target_model, target_train_data, target_test_data, 1, TEST_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wlEUjkOWPPq"
      },
      "source": [
        "#calculate the prediction\n",
        "from sklearn.metrics import accuracy_score\n",
        "ypred=target_model[0].predict_classes(xtrain_agg)\n",
        "accuracy_score(label(ytrain_agg), ypred)\n",
        "#print(eps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNsvN7AyWUSm"
      },
      "source": [
        "##attack classifier:RF\n",
        "def get_trained_RF_models(train_data, test_data, num_models):\n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "    (x_train, y_train), (x_test, y_test) = train_data, test_data\n",
        "    models = []\n",
        "    score=np.zeros(num_models)\n",
        "    #RF=RandomForestClassifier(random_state=0)\n",
        "    for i in range(num_models):\n",
        "        print('Training RF model : ', i)\n",
        "        models.append(RandomForestClassifier(random_state=0))\n",
        "        models[i].fit(x_train[i], y_train[i])\n",
        "        score[i] = models[i].score(x_test[i],y_test[i])\n",
        "        print('Random Forest model ', i, 'score : ',score)\n",
        "    return models, score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-tcmbBcWXOy"
      },
      "source": [
        "#generate the report #svm model\n",
        "def get_score_svm_models(models, test_data):\n",
        "    from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "    from sklearn.metrics import average_precision_score\n",
        "    (x_test, y_true) = test_data\n",
        "    acc_scores = []\n",
        "    pre_scores = []\n",
        "    rec_scores = []\n",
        "    for i in range(len(models)):\n",
        "        y_pred = models[i].predict(x_test[i])\n",
        "        # _LOG_PRINT(y_pred)\n",
        "        acc_scores.append(accuracy_score(y_true[i], y_pred))\n",
        "        pre_scores.append(average_precision_score(y_true[i], y_pred))\n",
        "        rec_scores.append(recall_score(y_true[i], y_pred))\n",
        "    return np.average(acc_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-Pw6yXDaKbt"
      },
      "source": [
        "def get_trained_svm_models(train_data, test_data, num_models):\n",
        "    from sklearn import svm\n",
        "    (x_train, y_train), (x_test, y_test) = train_data, test_data\n",
        "    models = []\n",
        "    k=0\n",
        "    for i in range(num_models):\n",
        "        print('Training svm model : ', i)\n",
        "        models.append(svm.SVC(gamma='scale',kernel='linear',verbose=VERBOSE))\n",
        "        models[i].fit(x_train[i], y_train[i])\n",
        "        score = models[i].score(x_test[i],y_test[i])\n",
        "        print('SVM model ', i, 'score : ',score)\n",
        "        k=k+1\n",
        "        print('this is executable',k)\n",
        "    return models,score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juDwXPpuWa7e"
      },
      "source": [
        "attack_model,scores = get_trained_svm_models(attack_train,attack_test, 10)\n",
        "#scores=get_score_svm_models(attack_model,attack_train)\n",
        "np.average(scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3ZCx1ASWeNW"
      },
      "source": [
        "def calc_labelAcc(Model,xtest,ytest):\n",
        "  from sklearn.metrics import accuracy_score\n",
        "  ypred=Model.predict_classes(xtest)\n",
        "  acc=accuracy_score(label(ytest), ypred)\n",
        "  return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYI0PS_wWhff"
      },
      "source": [
        "def lk(tn,tp,fn,fp):\n",
        "  tpr=(tp/(tp+fn))\n",
        "  fpr=(fp/(fp+tn))\n",
        "  return tpr-fpr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uv53n_Jpxi5Q"
      },
      "source": [
        "tn,tp,fn,fp=get_leakage(attack_model, attack_test)\n",
        "lk(tn,tp,fn,fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HnFYuyYnsWw"
      },
      "source": [
        "Input Perturbation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bl0-NDsfnuSL"
      },
      "source": [
        "def Input_pert_train(epi,xtrain,ytrain,xtest,ytest):\n",
        "  delta=.00001\n",
        "  c=1\n",
        "  L=250\n",
        "  E=100\n",
        "  n=15000\n",
        "  T=steps(L,E,n)\n",
        "  print(T)\n",
        "  G=3*np.log(n)\n",
        "  z1=c*(G**2)*T*np.log(1/delta)\n",
        "  z2=n*(n-1)*(epi**2)\n",
        "  sigma=z1/z2\n",
        "  nsi=np.random.normal(loc=0.0, scale=sigma)\n",
        "  #print(nsi)\n",
        "  X_in=xtrain+nsi\n",
        "  target_train_aggin = (X_in,ytrain)\n",
        "  target_test_aggin = (xtest,ytest)\n",
        "  target_train_datain, target_test_datain = sample_data(target_train_aggin, target_test_aggin, 1)\n",
        "  return X_in,target_train_datain,target_test_datain"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HK1otpgJn5FZ"
      },
      "source": [
        "ep=[0.01, 0.1,1,10,100,1000,10000]\n",
        "p=len(ep)\n",
        "lin=np.zeros(p)\n",
        "tn0=np.zeros(p)\n",
        "tp0=np.zeros(p)\n",
        "fn0=np.zeros(p)\n",
        "fp0=np.zeros(p)\n",
        "\n",
        "for i in range(len(ep)):\n",
        "     target_model=[]\n",
        "     X_in,target_train_datain,target_test_datain=Input_pert_train(ep[i],xtrain,ytrain,xtest,ytest)\n",
        "     in_model=Model(X_in,ytrain,xtest,ytest)\n",
        "     target_model.append(in_model)\n",
        "     attack_test_agg=get_attack_dataset(target_model, target_train_datain, target_test_datain, 1, TEST_SIZE)\n",
        "     tn0[i],tp0[i],fn0[i],fp0[i]=get_leakage(attack_model, attack_test_agg)\n",
        "     lin[i]=max(0,lk(tn0[i],tp0[i],fn0[i],fp0[i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_T9Gagbn6D4"
      },
      "source": [
        "print(lin)\n",
        "print(tp0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHYHwJfLWld2"
      },
      "source": [
        "Gradient"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewCyT65RXGgM"
      },
      "source": [
        "p=len(ep)\n",
        "lgd=np.zeros(p)\n",
        "tn2=np.zeros(p)\n",
        "tp2=np.zeros(p)\n",
        "fn2=np.zeros(p)\n",
        "fp2=np.zeros(p)\n",
        "for i in range(len(ep)):\n",
        "     target_model=[]\n",
        "     modelSGD=globals()['model_sgd%s' % i]\n",
        "     target_model.append(modelSGD)\n",
        "     attack_test_agg=get_attack_dataset(target_model, target_train_data, target_test_data, 1, TEST_SIZE)\n",
        "     tn2[i],tp2[i],fn2[i],fp2[i]=get_leakage(attack_model, attack_test_agg)\n",
        "     lgd[i]=lk(tn2[i],tp2[i],fn2[i],fp2[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZtEdYtNXKAv"
      },
      "source": [
        "print(lgd)\n",
        "print(tp2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bi3F6yHNkLcZ"
      },
      "source": [
        "def PATE(ep,xtest1,ytest1):\n",
        "            sum_t=np.zeros([len(ytest1),10])\n",
        "            for k1 in range(100):\n",
        "               sum=PATE_sum(xtest1,ytest1)\n",
        "               sum_s=np.zeros(sum.shape)\n",
        "               sum_n=sum\n",
        "               sum_s=sum_n+np.random.laplace(loc=0.0, scale=1/ep)\n",
        "               sum_t=sum_s+sum_t\n",
        "\n",
        "            return sum_t/100\n",
        "          \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0COEz5yoBdi"
      },
      "source": [
        "def PATE_sum(xtest1,ytest1):\n",
        "  sum=np.zeros([len(ytest1),10])\n",
        "  for iter in range(M):\n",
        "    New_model=globals()['model_pate%s' % iter]\n",
        "    y=np.zeros([len(ytest1),10])\n",
        "    y=New_model.predict_proba(xtest1)\n",
        "  #print(y.shape)\n",
        "    y=vote(y)\n",
        "    sum=y+sum\n",
        "  return sum/M"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Okrn4AYNoFKh"
      },
      "source": [
        "p=len(ep)\n",
        "lpate=np.zeros(p)\n",
        "tn4=np.zeros(p)\n",
        "tp4=np.zeros(p)\n",
        "fn4=np.zeros(p)\n",
        "fp4=np.zeros(p)\n",
        "for i in range(7):\n",
        "     ytrain_pred=PATE(ep[i],xtrain_agg,ytrain_agg)\n",
        "     ytest_pred=PATE(ep[i],xtest_agg,ytest_agg)\n",
        "     attack_test_agg=get_attack_dataset_combined(xtrain_agg,xtest_agg,ytrain_pred,ytrain_agg,ytest_agg,ytest_pred)\n",
        "     print(\"complete\")\n",
        "     tn4[i],tp4[i],fn4[i],fp4[i]=get_leakage(attack_model, attack_test_agg)\n",
        "     lpate[i]=lk(tn4[i],tp4[i],fn4[i],fp4[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zwDtc8HoLr6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15d06f03-234e-40c7-9136-819adcb25fbf"
      },
      "source": [
        "print(lpate)\n",
        "print(tp4)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.02746667, 0.03206667, 0.03206667, 0.03206667, 0.03206667, 0.03206667, 0.03206667]\n",
            "[8124, 9186.0, 9186.0, 9186.0, 9186.0, 9186.0, 9186.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jp-hK5Fsk1j1"
      },
      "source": [
        "[0.02746667, 0.03206667, 0.03206667, 0.03206667, 0.03206667, 0.03206667, 0.03206667]\n",
        "[8124, 9186.0, 9186.0, 9186.0, 9186.0, 9186.0, 9186.0]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eeTd2Qnoc5F"
      },
      "source": [
        "import pandas as pd \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns #grafikleştirme için\n",
        "import matplotlib.pyplot as plt \n",
        "from google.colab import files\n",
        "test1 = plt.figure()\n",
        "#plt.semilogx(ep,lobj,color=\"black\",marker='3',label='Objective Perturbation',linewidth=1.5)\n",
        "plt.semilogx(ep,lin,color=\"black\",marker='^',label='Input Perturbation',linewidth=1.5)\n",
        "plt.semilogx(ep,lgd,color=\"red\",marker='*',label='Gradient Perturbation',linewidth=1.5)\n",
        "#plt.semilogx(ep,lout,color=\"black\",marker='+',linestyle=\"--\",label='Output Perturbation',linewidth=1.5)\n",
        "plt.semilogx(ep,lpate,color=\"brown\",marker='o',label='Prediction Perturbation',linewidth=1.5)\n",
        "#plt.semilogx(ep,non_p-acc_in,color=\"orange\",marker='.',linestyle=\"--\",label='Input',linewidth=1.5)\n",
        "#plt.plot(ep,non_p,color=\"red\",marker='*',linestyle=\"--\",label='Non-Private Model',linewidth=2)\n",
        "plt.legend(loc=1,fontsize=12)\n",
        "plt.xlabel(\"Privacy Budget($\\epsilon$)\",fontsize=13)\n",
        "plt.ylabel(\"Privacy Leakage\",fontsize=15)\n",
        "#plt.xscale('symlog', linthreshy=0.1)\n",
        "#plt.ylim([-.1,1])\n",
        "plt.xticks(size = 10)\n",
        "plt.yticks(size = 8)\n",
        "plt.ylim([-.03,.7])\n",
        "#y.set_color(\"black\")\n",
        "plt.rcParams[\"axes.edgecolor\"] = \"black\"\n",
        "plt.rcParams[\"axes.linewidth\"] = 1\n",
        "plt.rcParams['axes.facecolor'] = 'white'\n",
        "#test1.set_facecolor('white')\n",
        "test1.show()\n",
        "test1.savefig('DNN_perturb_leakage_cifar.pdf')\n",
        "files.download('DNN_perturb_leakage_cifar.pdf')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcUtwIRDoemi"
      },
      "source": [
        "import pandas as pd \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns #grafikleştirme için\n",
        "import matplotlib.pyplot as plt \n",
        "from google.colab import files\n",
        "test1 = plt.figure()\n",
        "#plt.semilogx(ep,tp1,color=\"black\",marker='D',label='Objective Perturbation',linewidth=1.5)\n",
        "plt.semilogx(ep,tp0,color=\"black\",marker='^',label='Input Perturbation',linewidth=1.5)\n",
        "plt.semilogx(ep,tp2,color=\"red\",marker='*',label='Gradient Perturbation',linewidth=1.5)\n",
        "#plt.semilogx(ep,tp3,color=\"black\",marker='<',label='Output Perturbation',linewidth=1.5)\n",
        "plt.semilogx(ep,tp4,color=\"brown\",marker='o',label='Prediction Perturbation',linewidth=1.5)\n",
        "#plt.semilogx(ep,non_p-acc_in,color=\"orange\",marker='.',linestyle=\"--\",label='Input',linewidth=1.5)\n",
        "#plt.plot(ep,non_p,color=\"red\",marker='*',linestyle=\"--\",label='Non-Private Model',linewidth=2)\n",
        "plt.legend(loc=1,fontsize=10)\n",
        "plt.xlabel(\"Privacy Budget($\\epsilon$)\",fontsize=13)\n",
        "plt.ylabel(\"True Revealed Data (True Positive)\",fontsize=12)\n",
        "#plt.xscale('symlog', linthreshy=0.1)\n",
        "#plt.ylim([-.1,1])\n",
        "plt.xticks(size = 10)\n",
        "plt.yticks(size = 8)\n",
        "plt.ylim([0,20000])\n",
        "#y.set_color(\"black\")\n",
        "plt.rcParams[\"axes.edgecolor\"] = \"black\"\n",
        "plt.rcParams[\"axes.linewidth\"] = 1\n",
        "plt.rcParams['axes.facecolor'] = 'white'\n",
        "#test1.set_facecolor('white')\n",
        "test1.show()\n",
        "test1.savefig('DNN_perturb_tp_cifar.pdf')\n",
        "files.download('DNN_perturb_tp_cifar.pdf')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}